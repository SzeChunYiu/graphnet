{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f34a08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating dataset for 3 particles per event ===\n",
      "  File 0 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 1 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 2 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 3 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 4 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 5 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 6 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 7 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 8 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 9 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 10 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 11 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 12 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 13 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 14 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 15 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 16 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 17 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 18 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 19 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 20 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 21 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 22 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 23 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 24 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 25 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 26 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 27 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 28 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 29 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 30 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 31 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 32 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 33 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 34 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 35 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 36 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 37 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 38 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 39 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 40 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 41 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 42 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 43 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 44 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 45 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 46 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 47 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 48 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n",
      "  File 49 written to /home/billy/graphnet/HIBEAM/large_training_data_250k/particles_3 (accepted 5000 events)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "BASE_DIR = Path(\"large_training_data\")\n",
    "\n",
    "R_INNER, R_OUTER = 0.22, 0.32\n",
    "N_LAYERS = 10\n",
    "LAYER_RADII = np.linspace(R_INNER, R_OUTER, N_LAYERS)\n",
    "TPC_HALF_LENGTH = 0.516 / 2.0\n",
    "\n",
    "MUON_MASS = 105.658  # MeV\n",
    "C = 299_792_458.0    # m/s\n",
    "\n",
    "N_FILES = 50\n",
    "EVENTS_PER_FILE = 5000\n",
    "PARTICLES_CHOICES = [3]  # [2,3, 4, 5 , 10, 20, 30]\n",
    "EPS = 1e-9\n",
    "MIN_HITS_PER_EVENT = 3            # <── require at least this many hits\n",
    "\n",
    "for n_particles in PARTICLES_CHOICES:\n",
    "    print(f\"\\n=== Generating dataset for {n_particles} particles per event ===\")\n",
    "\n",
    "    # separate output directory for this case\n",
    "    DATA_DIR = BASE_DIR / f\"particles_{n_particles}\"\n",
    "    (DATA_DIR / \"pulses\").mkdir(parents=True, exist_ok=True)\n",
    "    (DATA_DIR / \"truth\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    global_event_id = 0\n",
    "\n",
    "    for index in range(N_FILES):\n",
    "        pulses_rows, truth_rows = [], []\n",
    "\n",
    "        accepted = 0\n",
    "        attempts = 0\n",
    "        max_attempts = EVENTS_PER_FILE * 50  # safety guard\n",
    "\n",
    "        while accepted < EVENTS_PER_FILE and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            # --- vertex -------------------------------------------------\n",
    "            r   = rng.uniform(0, 0.2)\n",
    "            phi = rng.uniform(0, 2*np.pi)\n",
    "            x0, y0 = r*np.cos(phi), r*np.sin(phi)\n",
    "            z0  = rng.uniform(-0.004, 0.004)\n",
    "\n",
    "            # collect hits for THIS event locally first\n",
    "            event_pulses = []\n",
    "\n",
    "            # --- particles ----------------------------------------------\n",
    "            for _ in range(n_particles):\n",
    "                phi   = rng.uniform(0, 2*np.pi)\n",
    "                theta = rng.uniform(0, np.pi/4)\n",
    "                direction = np.array([\n",
    "                    np.cos(phi) * np.cos(theta),\n",
    "                    np.sin(phi) * np.cos(theta),\n",
    "                    rng.choice([-1, 1]) * np.sin(theta),\n",
    "                ])\n",
    "\n",
    "                KE     = rng.uniform(200, 800)\n",
    "                gamma  = (KE + MUON_MASS) / MUON_MASS\n",
    "                beta   = np.sqrt(1 - 1 / gamma**2)\n",
    "                speed  = beta * C\n",
    "\n",
    "                dz = direction[2]\n",
    "                if   dz > EPS: s_endcap = (TPC_HALF_LENGTH - z0) / dz\n",
    "                elif dz < -EPS: s_endcap = (-TPC_HALF_LENGTH - z0) / dz\n",
    "                else:           s_endcap = np.inf\n",
    "                if s_endcap <= EPS:\n",
    "                    continue\n",
    "\n",
    "                dx, dy = direction[0], direction[1]\n",
    "                a = dx*dx + dy*dy\n",
    "                for r_layer in LAYER_RADII:\n",
    "                    b = 2*(x0*dx + y0*dy)\n",
    "                    c0 = x0*x0 + y0*y0 - r_layer*r_layer\n",
    "                    if a <= EPS:\n",
    "                        continue\n",
    "                    disc = b*b - 4*a*c0\n",
    "                    if disc <= 0:\n",
    "                        continue\n",
    "\n",
    "                    sqrt_disc = np.sqrt(disc)\n",
    "                    s1 = (-b - sqrt_disc) / (2*a)\n",
    "                    s2 = (-b + sqrt_disc) / (2*a)\n",
    "                    candidates = [s for s in (s1, s2) if s > EPS]\n",
    "                    if not candidates:\n",
    "                        continue\n",
    "\n",
    "                    s_layer = min(candidates)\n",
    "                    if s_layer - s_endcap > EPS:\n",
    "                        continue\n",
    "\n",
    "                    x_hit = x0 + dx*s_layer\n",
    "                    y_hit = y0 + dy*s_layer\n",
    "                    z_hit = z0 + dz*s_layer\n",
    "                    if abs(z_hit) - TPC_HALF_LENGTH > 1e-6:\n",
    "                        continue\n",
    "\n",
    "                    t_hit = (s_layer / speed) * 1e9  # ns\n",
    "                    event_pulses.append({\n",
    "                        \"event_id\": global_event_id,   # tentative; finalized on accept\n",
    "                        \"dom_x\": x_hit,\n",
    "                        \"dom_y\": y_hit,\n",
    "                        \"dom_z\": z_hit,\n",
    "                        \"dom_t\": t_hit,\n",
    "                    })\n",
    "\n",
    "            # ---------- accept / reject the event ------------------------\n",
    "            if len(event_pulses) >= MIN_HITS_PER_EVENT:\n",
    "                # finalize event_id into rows and append\n",
    "                for row in event_pulses:\n",
    "                    row[\"event_id\"] = global_event_id\n",
    "                pulses_rows.extend(event_pulses)\n",
    "\n",
    "                truth_rows.append({\n",
    "                    \"event_id\": global_event_id,\n",
    "                    \"position_x\": x0,\n",
    "                    \"position_y\": y0,\n",
    "                    \"position_z\": z0,\n",
    "                })\n",
    "\n",
    "                global_event_id += 1\n",
    "                accepted += 1\n",
    "            # else: reject and try another event (no writes, no id increment)\n",
    "\n",
    "        if accepted < EVENTS_PER_FILE:\n",
    "            print(f\"  [WARN] Only accepted {accepted}/{EVENTS_PER_FILE} events in file {index} after {attempts} attempts.\")\n",
    "\n",
    "        pd.DataFrame(pulses_rows).to_parquet(DATA_DIR/\"pulses\"/f\"pulses_{index}.parquet\")\n",
    "        pd.DataFrame(truth_rows ).to_parquet(DATA_DIR/\"truth\"/f\"truth_{index}.parquet\")\n",
    "        print(f\"  File {index} written to\", DATA_DIR.resolve(), f\"(accepted {accepted} events)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aac501a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating validation dataset for 3 particles/event ===\n",
      "  File 0 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 1 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 2 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 3 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 4 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 5 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 6 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 7 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 8 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 9 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 10 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 11 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 12 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 13 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 14 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 15 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 16 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 17 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 18 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 19 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 20 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 21 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 22 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 23 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 24 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 25 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 26 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 27 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 28 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 29 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 30 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 31 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 32 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 33 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 34 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 35 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 36 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 37 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 38 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 39 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 40 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 41 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 42 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 43 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 44 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 45 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 46 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 47 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 48 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "  File 49 written to /home/billy/graphnet/HIBEAM/large_validation_data/vertex_random/particles_3 (accepted 500 events)\n",
      "\n",
      "All validation datasets generated.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# configuration\n",
    "# ------------------------------------------------------------------\n",
    "BASE_DIR = Path(\"large_validation_data/vertex_random/\")            # base output folder\n",
    "PARTICLES_CHOICES = [3] #2, 3, 4, 5              # one dataset per value\n",
    "\n",
    "N_FILES = 50\n",
    "EVENTS_PER_FILE = 500\n",
    "MIN_HITS_PER_EVENT = 3                        # <── require at least this many hits\n",
    "target_radius = 0.2\n",
    "\n",
    "# detector geometry\n",
    "R_INNER, R_OUTER = 0.22, 0.32      # m\n",
    "N_LAYERS = 5\n",
    "LAYER_RADII = np.linspace(R_INNER, R_OUTER, N_LAYERS)\n",
    "TPC_HALF_LENGTH = 0.516 / 2.0      # m\n",
    "\n",
    "MUON_MASS = 105.658                # MeV\n",
    "C = 299_792_458.0                  # m/s\n",
    "EPS = 1e-9\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# generate one dataset per particle multiplicity\n",
    "# ------------------------------------------------------------------\n",
    "for n_particles in PARTICLES_CHOICES:\n",
    "    print(f\"\\n=== Generating validation dataset for {n_particles} particles/event ===\")\n",
    "\n",
    "    # separate output directory for this case\n",
    "    DATA_DIR = BASE_DIR / f\"particles_{n_particles}\"\n",
    "    (DATA_DIR / \"pulses\").mkdir(parents=True, exist_ok=True)\n",
    "    (DATA_DIR / \"truth\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # independent, reproducible RNG per dataset\n",
    "    rng = np.random.default_rng(1 + n_particles)\n",
    "\n",
    "    global_event_id = 0  # counter spanning all files (reset per dataset)\n",
    "\n",
    "    for index in range(N_FILES):\n",
    "        pulses_rows, truth_rows = [], []\n",
    "\n",
    "        accepted = 0\n",
    "        attempts = 0\n",
    "        max_attempts = EVENTS_PER_FILE * 50  # safety guard to avoid infinite loops\n",
    "\n",
    "        while accepted < EVENTS_PER_FILE and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            # ---------- vertex -----------------------------------------\n",
    "            r   = rng.uniform(0, target_radius)\n",
    "            phi = rng.uniform(0, 2*np.pi)\n",
    "            x0, y0 = r*np.cos(phi), r*np.sin(phi)\n",
    "            z0  = rng.uniform(-0.004, 0.004)                # ±0.4 cm\n",
    "\n",
    "            # collect hits for THIS event locally first\n",
    "            event_pulses = []\n",
    "\n",
    "            # ---------- particles --------------------------------------\n",
    "            for _ in range(n_particles):\n",
    "                # direction\n",
    "                phi   = rng.uniform(0, 2*np.pi)\n",
    "                theta = rng.uniform(0, np.pi/4)\n",
    "                direction = np.array([\n",
    "                    np.cos(phi) * np.cos(theta),\n",
    "                    np.sin(phi) * np.cos(theta),\n",
    "                    rng.choice([-1, 1]) * np.sin(theta),\n",
    "                ])\n",
    "\n",
    "                # speed from kinetic energy\n",
    "                KE = rng.uniform(200, 800)      # MeV\n",
    "                gamma = (KE + MUON_MASS) / MUON_MASS\n",
    "                beta = np.sqrt(1 - 1/gamma**2)\n",
    "                speed = beta * C\n",
    "\n",
    "                # distance to nearest endcap\n",
    "                dz = direction[2]\n",
    "                if   dz > EPS: s_endcap = (TPC_HALF_LENGTH - z0) / dz\n",
    "                elif dz < -EPS: s_endcap = (-TPC_HALF_LENGTH - z0) / dz\n",
    "                else:           s_endcap = np.inf\n",
    "                if s_endcap <= EPS:\n",
    "                    continue\n",
    "\n",
    "                # hit positions for each cylindrical layer\n",
    "                dx, dy = direction[0], direction[1]\n",
    "                a = dx*dx + dy*dy\n",
    "                for r_layer in LAYER_RADII:\n",
    "                    b = 2*(x0*dx + y0*dy)\n",
    "                    c = x0*x0 + y0*y0 - r_layer*r_layer\n",
    "                    if a <= EPS:\n",
    "                        continue\n",
    "                    disc = b*b - 4*a*c\n",
    "                    if disc <= 0:\n",
    "                        continue\n",
    "                    sqrt_disc = np.sqrt(disc)\n",
    "                    s_candidates = [\n",
    "                        (-b - sqrt_disc) / (2*a),\n",
    "                        (-b + sqrt_disc) / (2*a),\n",
    "                    ]\n",
    "                    s_layer = min([s for s in s_candidates if s > EPS], default=None)\n",
    "                    if s_layer is None or s_layer - s_endcap > EPS:\n",
    "                        continue\n",
    "\n",
    "                    x_hit = x0 + dx*s_layer\n",
    "                    y_hit = y0 + dy*s_layer\n",
    "                    z_hit = z0 + dz*s_layer\n",
    "                    if abs(z_hit) - TPC_HALF_LENGTH > 1e-6:\n",
    "                        continue\n",
    "\n",
    "                    t_hit = (s_layer / speed) * 1e9  # ns\n",
    "                    event_pulses.append({\n",
    "                        \"event_id\": global_event_id,  # tentative; finalized on accept\n",
    "                        \"dom_x\": x_hit,\n",
    "                        \"dom_y\": y_hit,\n",
    "                        \"dom_z\": z_hit,\n",
    "                        \"dom_t\": t_hit,\n",
    "                    })\n",
    "\n",
    "            # ---------- accept / reject the event ----------------------\n",
    "            if len(event_pulses) >= MIN_HITS_PER_EVENT:\n",
    "                # stamp the final event_id and append\n",
    "                for row in event_pulses:\n",
    "                    row[\"event_id\"] = global_event_id\n",
    "                pulses_rows.extend(event_pulses)\n",
    "\n",
    "                truth_rows.append({\n",
    "                    \"event_id\": global_event_id,\n",
    "                    \"position_x\": x0,\n",
    "                    \"position_y\": y0,\n",
    "                    \"position_z\": z0,\n",
    "                })\n",
    "\n",
    "                global_event_id += 1\n",
    "                accepted += 1\n",
    "            # else: reject & retry\n",
    "\n",
    "        if accepted < EVENTS_PER_FILE:\n",
    "            print(f\"  [WARN] Only accepted {accepted}/{EVENTS_PER_FILE} events in file {index} after {attempts} attempts.\")\n",
    "\n",
    "        pd.DataFrame(pulses_rows).to_parquet(DATA_DIR/\"pulses\"/f\"pulses_{index}.parquet\")\n",
    "        pd.DataFrame(truth_rows ).to_parquet(DATA_DIR/\"truth\"/f\"truth_{index}.parquet\")\n",
    "        print(f\"  File {index} written to\", DATA_DIR.resolve(), f\"(accepted {accepted} events)\")\n",
    "\n",
    "print(\"\\nAll validation datasets generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24bf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TPC geometry (meters) — must match generation step\n",
    "# ------------------------------------------------------------\n",
    "R_INNER, R_OUTER = 0.22, 0.32\n",
    "TPC_HALF_LENGTH = 0.516 / 2.0\n",
    "\n",
    "def smear_hits_df(df: pd.DataFrame, sigma_cm: float, clip_to_tpc: bool = True,\n",
    "                  rng: Optional[np.random.Generator] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply 3D Gaussian smearing (per-axis) to dom_x, dom_y, dom_z (in meters).\n",
    "    sigma_cm is converted to meters internally.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(12345)\n",
    "    sigma_m = sigma_cm / 100.0\n",
    "\n",
    "    out = df.copy()\n",
    "    for comp in (\"dom_x\", \"dom_y\", \"dom_z\"):\n",
    "        if comp not in out.columns:\n",
    "            raise ValueError(f\"Column {comp} not found in pulses dataframe.\")\n",
    "        out[comp] = out[comp] + rng.normal(loc=0.0, scale=sigma_m, size=len(out))\n",
    "\n",
    "    if clip_to_tpc:\n",
    "        # Clip cylindrical radius while preserving the azimuth\n",
    "        r = np.sqrt(out[\"dom_x\"]**2 + out[\"dom_y\"]**2)\n",
    "        theta = np.arctan2(out[\"dom_y\"], out[\"dom_x\"])\n",
    "        r = np.clip(r, R_INNER, R_OUTER)\n",
    "        out[\"dom_x\"] = r * np.cos(theta)\n",
    "        out[\"dom_y\"] = r * np.sin(theta)\n",
    "        # Clip z\n",
    "        out[\"dom_z\"] = np.clip(out[\"dom_z\"], -TPC_HALF_LENGTH, TPC_HALF_LENGTH)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _list_datasets(in_base: Path):\n",
    "    \"\"\"\n",
    "    Detect input layout.\n",
    "    Returns a list of tuples: (dataset_tag, pulses_dir, truth_dir)\n",
    "\n",
    "    - Flat layout:\n",
    "        dataset_tag == \"\" (empty)\n",
    "        pulses: in_base/pulses\n",
    "        truth : in_base/truth\n",
    "\n",
    "    - Per-multiplicity layout:\n",
    "        dataset_tag == \"particles_<N>\"\n",
    "        pulses: in_base/particles_<N>/pulses\n",
    "        truth : in_base/particles_<N>/truth\n",
    "    \"\"\"\n",
    "    in_base = Path(in_base)\n",
    "    flat_pulses = in_base / \"pulses\"\n",
    "    flat_truth  = in_base / \"truth\"\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    # Prefer per-multiplicity if present\n",
    "    particle_dirs = sorted([p for p in in_base.glob(\"particles_*\") if p.is_dir()])\n",
    "    if particle_dirs:\n",
    "        for pdir in particle_dirs:\n",
    "            p_pulses = pdir / \"pulses\"\n",
    "            p_truth  = pdir / \"truth\"\n",
    "            if p_pulses.exists() and list(p_pulses.glob(\"pulses_*.parquet\")) \\\n",
    "               and p_truth.exists()  and list(p_truth.glob(\"truth_*.parquet\")):\n",
    "                datasets.append((pdir.name, p_pulses, p_truth))\n",
    "\n",
    "    # Otherwise fall back to flat\n",
    "    elif flat_pulses.exists() and list(flat_pulses.glob(\"pulses_*.parquet\")) \\\n",
    "         and flat_truth.exists()  and list(flat_truth.glob(\"truth_*.parquet\")):\n",
    "        datasets.append((\"\", flat_pulses, flat_truth))\n",
    "\n",
    "    if not datasets:\n",
    "        raise FileNotFoundError(\n",
    "            \"No input datasets found.\\n\"\n",
    "            \"Expected either:\\n\"\n",
    "            \"  - flat: <in_base>/pulses/pulses_*.parquet and <in_base>/truth/truth_*.parquet\\n\"\n",
    "            \"  - per multiplicity: <in_base>/particles_*/{pulses,truth}/... .\"\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "def write_smeared_datasets(\n",
    "    resolutions_cm: Iterable[float],\n",
    "    in_base: str,\n",
    "    out_base: str,\n",
    "    overwrite: bool = False,\n",
    "    copy_truth: bool = True,\n",
    "    clip_to_tpc: bool = True,\n",
    "    seed: int = 12345,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each input dataset and each resolution (in cm), write smeared copies of pulses parquet files.\n",
    "\n",
    "    Input (auto-detected):\n",
    "      - Flat:           <in_base>/{pulses,truth}/\n",
    "      - Per multiplicity: <in_base>/particles_*/{pulses,truth}/\n",
    "\n",
    "    Output (mirrors input; inserts res_<v>cm level):\n",
    "      - Flat:           <out_base>/res_<v>cm/{pulses,truth}/\n",
    "      - Per multiplicity: <out_base>/<particles_X>/res_<v>cm/{pulses,truth}/\n",
    "    \"\"\"\n",
    "    in_base = Path(in_base)\n",
    "    out_base = Path(out_base)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    datasets = _list_datasets(in_base)\n",
    "\n",
    "    for tag, pulses_dir, truth_dir in datasets:\n",
    "        tag_msg = tag or \"(flat dataset)\"\n",
    "        print(f\"\\n=== Smearing dataset: {tag_msg} ===\")\n",
    "        parquet_files = sorted(pulses_dir.glob(\"pulses_*.parquet\"))\n",
    "        if not parquet_files:\n",
    "            print(f\"  No pulses found in {pulses_dir}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        for res in resolutions_cm:\n",
    "            res_tag = f\"res_{res}cm\"\n",
    "            # Build output path, mirroring input structure\n",
    "            out_root = (out_base / tag) if tag else out_base\n",
    "            out_dir = out_root / res_tag\n",
    "            out_pulses = out_dir / \"pulses\"\n",
    "            out_truth  = out_dir / \"truth\"\n",
    "            out_pulses.mkdir(parents=True, exist_ok=True)\n",
    "            out_truth.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            print(f\"  -> Writing {res_tag} to {out_dir.resolve()}\")\n",
    "\n",
    "            # Copy truth tables (unchanged)\n",
    "            if copy_truth:\n",
    "                for truth_path in sorted(truth_dir.glob(\"truth_*.parquet\")):\n",
    "                    dst = out_truth / truth_path.name\n",
    "                    if overwrite or not dst.exists():\n",
    "                        shutil.copy2(truth_path, dst)\n",
    "\n",
    "            # Create smeared pulses\n",
    "            for p in parquet_files:\n",
    "                df = pd.read_parquet(p)\n",
    "                df_sm = smear_hits_df(df, sigma_cm=float(res),\n",
    "                                      clip_to_tpc=clip_to_tpc, rng=rng)\n",
    "                dst = out_pulses / p.name\n",
    "                if overwrite or not dst.exists():\n",
    "                    df_sm.to_parquet(dst)\n",
    "\n",
    "            # Write simple metadata\n",
    "            meta = {\n",
    "                \"resolution_cm\": float(res),\n",
    "                \"sigma_m\": float(res) / 100.0,\n",
    "                \"clip_to_tpc\": bool(clip_to_tpc),\n",
    "                \"seed\": int(seed),\n",
    "                \"source_dir\": str(pulses_dir.resolve()),\n",
    "                \"dataset_tag\": tag,\n",
    "            }\n",
    "            pd.Series(meta).to_json(out_dir / \"metadata.json\", indent=2)\n",
    "\n",
    "    print(\"\\nAll smearing tasks complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a987e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Smearing dataset: particles_3 ===\n",
      "  -> Writing res_0cm to /home/billy/graphnet/HIBEAM/large_training_data_smeared/particles_3/res_0cm\n",
      "\n",
      "All smearing tasks complete.\n",
      "\n",
      "=== Smearing dataset: particles_3 ===\n",
      "  -> Writing res_0cm to /home/billy/graphnet/HIBEAM/large_validation_data_smeared/vertex_random/particles_3/res_0cm\n",
      "\n",
      "All smearing tasks complete.\n"
     ]
    }
   ],
   "source": [
    "write_smeared_datasets([0], in_base=\"large_training_data/\", out_base=\"large_training_data_smeared\") #,0.5,1.0\n",
    "#0, 0.01, 0.1, 0.5, 1.0 \n",
    "write_smeared_datasets([0], in_base=\"large_validation_data/vertex_random/\", out_base=\"large_validation_data_smeared/vertex_random/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25789746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (graphnet_env)",
   "language": "python",
   "name": "graphnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
